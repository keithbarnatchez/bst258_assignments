---
title: "Problem Set #2"
subtitle: "BST 258: Causal Inference -- Theory and Practice"
author: "Keith Barnatchez"
date: ""
format:
  pdf:
    documentclass: scrartcl
    papersize: letter
    fontsize: 11pt
    geometry:
      - margin=1in
      - heightrounded
    number-sections: false
    colorlinks: true
    link-citations: true
    callout-appearance: simple
    callout-icon: false
    # figure options
    fig-width: 6
    fig-asp: 0.618
    fig-cap-location: bottom
    # code block options
    code-line-numbers: false
    code-block-bg: false
    highlight-style: nord
---

\newcommand{\E}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}

```{r, echo=FALSE, include=FALSE}
library(tidyverse)
library(foreach)
library(doParallel)
```

**Link to the repo for this HW**: `https://github.com/keithbarnatchez/bst258_assignments`

# Question 1: Inverse Probability Weighting

## Part 1: Theory

1.  If the  conditional expectation/propensity score models are correctly specified, then...

- Property (1) by holds construction, as IPW weights are balancing weights. Notice this does *not* imply the IPW estimator is unbiased, as this requires conditional exchangeability.

- Under **positivity**, property (2) holds. We'll rigorously show this in Part 2 of this assignment, so I defer the explanation until then.

If we *additionally* assume conditional exchangeability, then...

- The IPW estimator is unbiased for the counterfactual mean $\E(Y^a)$.

- Meaning the mean in the psuedo-population equals the standardized mean in the original population, which equals $\E(Y^a)$.

2. Assume consistency, positivity, and conditional exchangeability. Then, notice

::: {.callout-warning title=""}
$$
\begin{aligned}
\E[Y^a] &= \E[\E(Y^a | L) ] \\
&= \E\left[ \frac{\mathbb{P}(A=a|L)}{\mathbb{P}(A=a|L)} \E(Y^a | L) \right] \\
&=\E\left[ \frac{\E(Y^a | L)\mathbb{E}[I(A=a|L)]}{\mathbb{P}(A=a|L)}  \right] \\
&=\E\left[ \frac{\E(Y I(A=a)| L)}{\mathbb{P}(A=a|L)}  \right] \\
&= \E\left[ \E\left(\frac{Y I(A=a)}{\mathbb{P}(A=a|L)}\bigg| L\right)  \right] \\
&=  \E\left(\frac{Y I(A=a)}{\mathbb{P}(A=a|L)}\right) 
\end{aligned}
$$
:::

The first line just applies iterated expectation. The second line holds since we just multiply and divide by a fixed quantity in the inner expectation. The third and fourth lines hold by conditional exchangeability and consistency, while the fifth is just re-arranging and the sixth by (reverse) iterated expectation.

\clearpage

## Part 2: Application

First, I load in and clean the data

```{r, warning=FALSE, message=FALSE}
# Data cleaning
nhefs <- read.csv('../data/nhefs.csv')

# Get set of covariates for treatment + outcome regressions
covs <- c('sex','age','race','education','smokeyrs','smokeintensity',
          'active','exercise','wt71') 

# Set outcome and treatment
outcome <- 'wt82_71' ; treatment <- 'qsmk'

# Set factor variables 
factor_vars <- c('sex','race','education',
                 'active','exercise')

# Filter out the vars we want and only keep complete cases
analysis_data <- nhefs %>%
  select(covs, outcome,treatment) %>%
  mutate_at(all_of(factor_vars), as.factor) %>%
  filter(!is.na(wt82_71))
```

Next, I estimate the propensity score model and construct the stabilized + unstabilized weights. Note that I assume by stabilized weights, which has so far not been defined in the course nor defined in the problem set, we mean the weights outlined in Hernán and Robins chapter 12:

$$
\frac{\mathbb{P}(A)}{\mathbb{P(A|L)}}
$$

<!-- $$ -->
<!-- \hat \psi_a^\text{Hajék} = \left(\frac{1}{n} \sum_{i=1}^n \frac{Y_i A_i}{\hat g(L_i)}\right) \bigg/ \left(\frac{1}{n} \sum_{i=1}^n \frac{A_i}{\hat g(L_i)}\right) -->
<!-- $$ -->

To be clear, I use "Hajék" versions of the stabilized and unstabilized weights in forming the corresponding IPW estimators:

$$
\begin{aligned}
\hat \psi_a^\text{Hajék} &= \frac{1}{n} \sum_{i=1}^n Y_i \color{blue} \frac{A_i}{\hat g(L_i)}\left(\frac{1}{n} \sum_{i=1}^n \frac{A_i}{\hat g(L_i)}\right)^{-1}\color{black} \ \ \ \text{(Unstabilized)} \\
\hat \psi_a^\text{Stb. Hajék} &= \frac{1}{n} \sum_{i=1}^n Y_i \color{blue} \frac{\hat{\mathbb{P}}(A_i)A_i}{\hat g(L_i)}\left(\frac{1}{n} \sum_{i=1}^n \frac{\hat{\mathbb{P}}(A_i)A_i}{\hat g(L_i)}\right)^{-1}\color{black} \ \ \ \text{(Stabilized)}
\end{aligned}
$$

I do this because (1) as explained in What If Technical Point 12.2, simply adding $\hat{\mathbb{P}}(A_i)$ into the numerator of the weights results in an inconsistent estimator, and (2) even though the unstabilized weight estimator is consistent with the Hajék or Horvitz Thompson form, it's more sensible to compare "compatible" versions of the estimators. Further, the Hajek IPW estimator is generally more efficient.

Above, the blue term above corresponds to the stabilized weight for each subject. As mentioned in Technical Point 12.1, the weights corresponding to the Hajék estimator are "stable" in the sense that they guarantee the estimator will respect the bounds of the outcome, and also have the property that the sum of the weights will equal the sample size, $n$.

```{r}
# Make formula for logistic regression (include every covariate and quadratic terms)
logistr <- as.formula(paste(treatment, '~', paste(covs, collapse = '+'),
                            '+ I(age^2) + I(wt71^2) + I(smokeintensity^2) + 
                            I(smokeyrs^2)'))

# Fit the propensity score model
ipw_mod <- glm(logistr, data = analysis_data, family = binomial)
ghat <- predict(ipw_mod, type = 'response')

# Form the IPW weights
analysis_data$ipw <- ifelse(analysis_data$qsmk == 1, 1/ghat, 1/(1-ghat))
analysis_data$stbl_ipw <- with(analysis_data,
                               ipw / mean(qsmk/ghat))
mean_qsmk <- mean(analysis_data$qsmk)
analysis_data$stbl_ipw <- ifelse(analysis_data$qsmk == 1,
                                 mean_qsmk/ghat, (1-mean_qsmk)/(1-ghat))
```

Then, I plot the weights

```{r,warning=FALSE, message=FALSE}
# Make df for plotting weights
plotdf <- with(analysis_data,
               data.frame(weight = c(ipw,stbl_ipw),
                                        type = rep(c('IPW','Stabilized IPW'),
                                                   each = nrow(analysis_data))))
# Plot the weights
plotdf %>%
  ggplot(aes(x = weight)) + facet_wrap(~type) +
  geom_histogram(fill='steelblue',color='black') +
  labs(title = 'Histogram of IPW Weights',
       x = 'IPW Weight',
       y = 'Frequency') +
  theme_minimal()
```

The stabilized weights are less disbursed than the unstabilized ones (providing intuition for why the corresponding estimator tends to be more efficient) and have an expectation of 1.

### Part b

With the weights constructed, we can form the IPW estimator. Since we're using Hajék estimators, we can estimate the ATE via a **weighted** simple linear regression, which was shown to be equivalent to the 

```{r}
get_ipw <- function(data, weight) {
  n <- nrow(data)
  
  # main_mod <- estimatr::lm_robust(wt82_71 ~ qsmk, data = data, weights = weight,
  #                                 se_type='HC1')
  
  main_mod <- survey::svyglm(wt82_71 ~ qsmk, design = survey::svydesign(ids = ~1,
                                                                       weights = ~weight,
                                                                       data = data))
  
  est <- main_mod$coef[2]
  se <- vcov(main_mod)[2,2]^0.5
  ci <- est + c(-1, 1) * 1.96 * se
  list(est = est, se = se, ci = ci, mod=main_mod)
}

# Applying the function to both weights
ipw_est <- get_ipw(analysis_data, analysis_data$ipw)
stipw_est <- get_ipw(analysis_data, analysis_data$stbl_ipw)
```

The estimates are included below:

```{r}
print(paste0('IPW estimate: ', round(ipw_est$est,3)))
print(paste0('Stabilized IPW estimate: ', round(stipw_est$est,3)))
```

The two estimates match exactly. This is actually expected, since the Hajék estimator is equivalent the estimate of $\beta_1$ from a **saturated** marginal structural model (model for the counterfactual outcome):

$$
\mathbb{E}(Y^a) = \beta_0 + \beta_1 a,
$$

with either the stabilized or unstabilized weights above. As noted in What If, for saturated MSMs the two weights will yield equivalent point estimates/SEs.

### Part c

Two methods (among a few) for estimating the variance of the IPW estimator are (1) bootstrapping, and (2) via a robust variance estimator that accounts for the uncertainty in the weight estimation. Above, I used the latter method to obtain SEs, which is implemented in the `lm_robust` function from the `estimatr` package, specifying the Huber-White estimator of the variance. It should be noted the corresponding intervals tend to be conservative.

The 95\% CIs are below:

```{r}
print(paste0('IPW 95% CI: (', round(ipw_est$ci[1],3),',',
             round(ipw_est$ci[2],3),')'))
print(paste0('Stabilized IPW 95% CI: (', round(stipw_est$ci[1],3),',',
             round(stipw_est$ci[2],3),')'))
```

We see the CIs also match. Again, this shouldn't be surprising, since we are implicitly fitting a saturated MSM, and we know stabilized/unstabilized weights yield equivalent estimates/SEs in this setting.

### Part d

To summarize:

```{r}
comparison_df <- data.frame(
  Method = c('IPW', 'Stabilized IPW'),
  Estimate = c(ipw_est$est, stipw_est$est),
  `Standard Error` = c(ipw_est$se, stipw_est$se),
  `Lower 95% CI` = c(ipw_est$ci[1], stipw_est$ci[1]),
  `Upper 95% CI` = c(ipw_est$ci[2], stipw_est$ci[2])
)

knitr::kable(comparison_df, 
             caption = "Comparison of IPW and Stabilized IPW Estimates",
             digits=2)
```

## Part 2: Doubly Robust Estimation

### Part a

I include my code for estimating $m_A(L)$, and extracting the predictions, below:

```{r}
outstr <- as.formula(paste(outcome, '~', paste(covs, collapse = '+'),
                        '+ qsmk + I(age^2) + I(wt71^2) + I(smokeintensity^2) + 
                         I(smokeyrs^2) + I(qsmk*smokeintensity)'))

# fit the outcome model
outcome_mod <- lm(outstr, data = analysis_data)

# Extract predictions under each treatment
m1 <- predict(outcome_mod, newdata = analysis_data %>% mutate(qsmk = 1))
m0 <- predict(outcome_mod, newdata = analysis_data %>% mutate(qsmk = 0))
```

### Part b

I obtain the point estimate of the DR estimator below:

```{r}
# Form the DR estimator
dr_est <- list() 
dr_est$est <- with(analysis_data,
                   ipw_est$est + 
                   mean(
                     (1-qsmk/ghat)*m1 - (1 - (1-qsmk)/(1-ghat))*m0
                   )
                   )
print(paste0('DR estimate: ', round(dr_est$est,3)))
```

### Part c

I compute the doubly-robust estimator's SE analytically by noting

$$
\begin{aligned}
\sqrt n(\hat \psi^\text{DR} - \psi) \overset{d}{\rightarrow}  \mathcal{N}\left(0, \mathbb{V}(\psi^\text{DR})\right)
\end{aligned}
$$


I compare the DR estimator to the IPW estimators with the code below: 

```{r}
dr_est$se <- sqrt(with(analysis_data,
                       var(
                         (1-qsmk/ghat)*m1 - (1 - (1-qsmk)/(1-ghat))*m0
                       )/nrow(analysis_data)))
dr_est$ci <- dr_est$est + c(-1,1)*1.96*dr_est$se

comparison_df <- data.frame(
  Method = c('IPW', 'Stabilized IPW', 'Doubly-Robust'),
  Estimate = c(ipw_est$est, stipw_est$est, dr_est$est),
  `Standard Error` = c(ipw_est$se, stipw_est$se,dr_est$se),
  `Lower 95% CI` = c(ipw_est$ci[1], stipw_est$ci[1],dr_est$ci[1]),
  `Upper 95% CI` = c(ipw_est$ci[2], stipw_est$ci[2],dr_est$ci[2])
)

knitr::kable(comparison_df, 
             caption = "Comparison of IPW, Stabilized IPW, and Doubly-Robust Estimates",
             digits=2)
```

We see the estimated standard error for the augmented estimator is notably smaller than either version of the IPW estimator, partly because it incorporates covariate information to model the outcome.

\clearpage

# Question 2: Standardization and Parametric G-Compuation

## Part 1: Theory

1. Assume positivity holds. We aim to show 

$$
\sum_l \E(Y | A=a, L=l) \times \PP(L=l) = \E \left( \frac{YI(A=a)}{g(L)} \right)
$$

First, define $g_a(L) = \E(I(A=a)|L)$, i.e. the conditional (on $L$) probability of $A=a$. Then, notice

$$
\begin{aligned}
\sum_l \E(Y | A=a, L=l) \times \PP(L=l) &= \E[\E(Y | A=a, L)] \\
&= \E\left[ \frac{g(L)}{g(L)} \E(Y | A=a, L)\right] \\
&= \E\left[ \frac{\E(I(A=a)|L)}{g(L)} \E(Y | A=a, L)\right] \\
&= \E\left[ \frac{\E(Y I(A=a) | L)}{g(L)} \right] \\
&= \E\left[ \E\left( \frac{Y I(A=a)}{g(L)} \right) \bigg| L\right] \\
&= \E\left[ \frac{Y I(A=a)}{g(L)}\right]
\end{aligned}
$$

Note the final term is the IPW among the uncensored individuals, and the first term on the right-hand side is the g-formula among the uncensored individuals. Positivity ensures the condtional expectations are well-defined and that dividing by $g_a(L)$ is a valid operation.

<!-- All this is telling us is that under positivity, the G-formula and IPW both are equivalent to the conditional mean of the outcome in the group that happened to have treatment value $a$ and that were uncensored. Note that this is **not** a causal quantity, which shouldn't be surprising given that we haven't made any causal assumptions. -->

2. Notice

$$
\begin{aligned}
\E(Y^a) &= \E\left(\E(Y^a | L)\right) \\
&= \E\left(\E(Y^a | A=a, L)\right) \\
&= \E\left(\E(Y | A=a, L)\right) \\
&= \sum_l \E(Y | A=a, L=l) \times \PP(L=l) \\
\end{aligned}
$$

The first line just applies iterated expectations. The second line uses conditional exchangeability, while the third line holds under consistency. The fourth line just expands the iterated expectation definition to make explicit that the third line is a more compact representation of the g-formula.

3. If the outcome model is correctly specified, but the propensity score model misspecified, then the DR estimator will still be consistent but typically have higher variance relative to the plug-in. So if one is reasonably confident in the outcome model but not confident in the propensity model, the plug-in may be the preferred choice. 

**If we assumed also nuisance models are non-parametric.** If one is reasonably confident in *both* estimators, or not confident about the outcome model (and reasonably confident about the propensity model), then the DR estimator would be preferred.

If one is reasonably confident that the true outcome model falls in a *parametric* family (super unrealistic, of course), then the plug-in would also be preferred -- the DR estimator attains the efficiency bound in non-parametric models, but not in parametric models, where the plug-in would be preferred. Again, this is a bit of a moot point because we'll rarely have reason to strongly believe that a parametric model truly captures the process governing the conditional outcome distribution.
 
## Part 2: Application

1.

- (a) First, I estimate the outcome model (though we already did this in the IPW section):

```{r}
outstr <- as.formula(paste(outcome, '~', paste(covs, collapse = '+'),
                            '+ qsmk + I(age^2) + I(wt71^2) + I(smokeintensity^2) + 
                            I(smokeyrs^2) + qsmk:smokeintensity'))

# Estimate outcome model
outcome_mod <- lm(outstr, data = analysis_data)
```

Then, I extract the appropriate predicted values to form the plug-in estimator. The question makes it unclear if we should just report a point estimate or also a standard error. For completeness I report both.

To estimate the SE, I use the `marginaleffects` package so as to correctly incorporate uncertainty in estimating $m_A(L)$. As discussed in Chapter 8 of [A Ride in Targeted Learning Theory](https://achambaz.github.io/tlride/8-naive-estimators.html), the empirical variance of the plug-in estimator is an *anti-conservative* estimate of the true asymptotic variance. `marginaleffects` uses the delta method to incorporate the uncertainty in estimating $m_A(L)$.

```{r}
# Extract predicted values
m1 <- predict(outcome_mod, newdata = analysis_data %>% mutate(qsmk = 1))
m0 <- predict(outcome_mod, newdata = analysis_data %>% mutate(qsmk = 0))

# Form the plug-in estimate
plugin_est <- list(est = mean(m1 - m0),
                   se=marginaleffects::avg_comparisons(outcome_mod, 
                                 variables=list(qsmk= c(0,1) ))$std.error)

print(paste0("The plug-in estimate is ", 
             round(plugin_est$est,2), " with a standard error of ", 
             round(plugin_est$se,2)))
```

- (b) We see the point estimates between IPW and the plug-in are similar, but that the plug-in has a smaller standard error. This isn't particularly surprising, as the plug-in estimator amounts to an average of differenced conditional expectation functions while IPW amounts to difference in weighted averages of the outcome. Since conditional expectations of an outcome will tend to have lower variance than the outcome itself, the asymptotic variance of G-comp in practice will tend to be lower and in turn it's not surprising the SE is lower in this application.

- (c) No! It **is** true that under consistency, positivity and conditional exchangeability (as well as correct specification of the appropriate nuisance models) that  IPW and G-computation estimators will be consistent for the same causal quantity (and under just positivity consistent for the same statistical, non-causal quantity). **However**, their asymptotic variances generally differ, and since each estimator is a unique random variable we should never expect that they give the exact same estimate in finite samples. 

2. 

- (a): *Doubly-robust* if often used to refer to a few different things, all with related but not quite the same meanings. In this context, people may refer to AIPW as doubly-robust because (1) it's consistent if either the outcome or propensity model is correctly specified, (2) its statistical rate is the product of the rates of the two (hence the "double") nuisance models (e.g. if the outcome and propensity models can be estimated at $n^{-1/4}$ rates, then AIPW's statistical rate would be $\sqrt n$), and (3) that the estimator itself is derived from semi-parametric theory (i.e. formed based on the efficient influence function of the corresponding statistical estimand).
 
- (b): We already implemented the DR estimator in Question 1 (the IPW section), so there is no need to do this again. I include the table originally provided in Question 1 below:

```{r}
knitr::kable(comparison_df %>% filter(Method=='Doubly-Robust'), 
             caption = "Doubly-Robust Estimate",
             digits = 2)
```

